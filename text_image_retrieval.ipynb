{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rgcs8MOo2_w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import requests\n",
        "import glob\n",
        "import pickle\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CLIP installation & import"
      ],
      "metadata": {
        "id": "SEoOzhq8pm-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip"
      ],
      "metadata": {
        "id": "a5Dcf4gZpluP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ALIGN installation & import"
      ],
      "metadata": {
        "id": "EgGnHd3pp9Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AlignProcessor, AlignModel"
      ],
      "metadata": {
        "id": "v-DCp5bhqA4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLIP-2 installation & import"
      ],
      "metadata": {
        "id": "PJ1hr51iqOLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install salesforce-lavis\n",
        "from lavis.models import load_model_and_preprocess"
      ],
      "metadata": {
        "id": "AXPSCpPNqVfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract class Retriever\n"
      ],
      "metadata": {
        "id": "UG5mKOwvqnbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Retriever:\n",
        "    def __init__(self) -> None:\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.image_IDs = []\n",
        "        self.image_encodings = None\n",
        "\n",
        "    def encode_images(self, images_paths ,out_dir=None, batch_size=50):\n",
        "        pass\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        pass\n",
        "\n",
        "    def compare_to_images(self, text):  # Returns a sorted list of tuples (cosine_similarity, Image_ID)\n",
        "        text_encoding = self.encode_text(text).T\n",
        "        cosine_similarities = torch.mm(self.image_encodings, text_encoding)\n",
        "        return sorted(zip(cosine_similarities.tolist(),self.image_IDs),reverse=True)\n",
        "\n",
        "    def load_encoded_images(self, directory, model_name):\n",
        "        # Load tensor from file\n",
        "        self.image_encodings = torch.load(directory + f'/{model_name}_image_encodings.pth')\n",
        "        # Load list from file\n",
        "        with open(directory + f'/{model_name}_images_IDs.pkl', 'rb') as f:\n",
        "            self.image_IDs = pickle.load(f)\n",
        "\n",
        "    def plot_top_images(self, sorted_list, n, querry):\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(1, n, figsize=(15, 5))\n",
        "\n",
        "        plt.suptitle(\"Querry: '\" + querry + \"'\", x=0.1, y=0.95, fontsize=16, ha='left')\n",
        "        # Display the top n images\n",
        "        for i in range(min(len(sorted_list), n)):\n",
        "            _, image_path = sorted_list[i]\n",
        "            image_path =  image_path\n",
        "            try:\n",
        "                # Display the image using matplotlib.image\n",
        "                img = mpimg.imread(image_path)\n",
        "                axes[i].imshow(img)\n",
        "                axes[i].axis('off')\n",
        "                axes[i].set_title(f\"Similarity: {round(sorted_list[i][0],4)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error displaying image {image_path}: {e}\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "RnglMRG_quCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPRetriever(Retriever):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
        "\n",
        "    def encode_images(self, images_paths ,out_dir=None, batch_size=50):\n",
        "        batches  = [images_paths[i:i+batch_size] for i in range(0, len(images_paths), batch_size)]\n",
        "        self.image_IDs = []\n",
        "        self.image_encodings = None\n",
        "        for batch in batches:\n",
        "            images = []\n",
        "\n",
        "            # Preprocess Images\n",
        "            for image_name in batch:\n",
        "                images.append(Image.open(image_name))\n",
        "                self.image_IDs.append(image_name)\n",
        "\n",
        "            preprocessed_images = []\n",
        "            for image in images:\n",
        "                image_input = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "                preprocessed_images.append(image_input)\n",
        "            preprocessed_images = torch.cat(preprocessed_images)\n",
        "            del images\n",
        "\n",
        "            # Encode Images\n",
        "            with torch.no_grad():\n",
        "                if self.image_encodings is None:\n",
        "                    self.image_encodings = self.model.encode_image(preprocessed_images)\n",
        "                else:\n",
        "                    self.image_encodings = torch.cat((self.image_encodings, self.model.encode_image(preprocessed_images)), dim=0)\n",
        "\n",
        "        self.image_encodings = F.normalize(self.image_encodings, p=2, dim=-1)\n",
        "        # Save if out_dir specified\n",
        "        if out_dir is not None and self.image_encodings is not None:\n",
        "            torch.save(self.image_encodings, out_dir + '/CLIP_image_encodings.pth')\n",
        "            with open(out_dir + '/CLIP_images_IDs.pkl', 'wb') as f:\n",
        "                pickle.dump(self.image_IDs, f)\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        # Preprocess text\n",
        "        text = clip.tokenize(text).to(self.device)\n",
        "\n",
        "        # Encode text\n",
        "        with torch.no_grad():\n",
        "            encoded_text = self.model.encode_text(text)\n",
        "        return encoded_text"
      ],
      "metadata": {
        "id": "NAgiMY5xq3Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ALIGNRetriever(Retriever):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.preprocess = AlignProcessor.from_pretrained(\"kakaobrain/align-base\")\n",
        "        self.model = AlignModel.from_pretrained(\"kakaobrain/align-base\")\n",
        "\n",
        "    def encode_images(self, images_paths ,out_dir=None, batch_size=50):\n",
        "        batches  = [images_paths[i:i+batch_size] for i in range(0, len(images_paths), batch_size)]\n",
        "        self.image_IDs = []\n",
        "        self.image_encodings = None\n",
        "        for batch in batches:\n",
        "            images = []\n",
        "\n",
        "            # Preprocess Images\n",
        "            for image_name in batch:\n",
        "                images.append(Image.open(image_name))\n",
        "                self.image_IDs.append(image_name)\n",
        "\n",
        "            preprocessed_input = self.preprocess(text=\"\",images=images, return_tensors=\"pt\")\n",
        "            del images\n",
        "\n",
        "            # Encode Images\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(**preprocessed_input)\n",
        "                if self.image_encodings is None:\n",
        "                    self.image_encodings = outputs.image_embeds\n",
        "                else:\n",
        "                    self.image_encodings = torch.cat((self.image_encodings, outputs.image_embeds), dim=0)\n",
        "\n",
        "        self.image_encodings = F.normalize(self.image_encodings, p=2, dim=-1)\n",
        "        # Save if out_dir specified\n",
        "        if out_dir is not None and self.image_encodings is not None:\n",
        "            torch.save(self.image_encodings, out_dir + '/ALIGN_image_encodings.pth')\n",
        "            with open(out_dir + '/ALIGN_images_IDs.pkl', 'wb') as f:\n",
        "                pickle.dump(self.image_IDs, f)\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        # Preprocess text\n",
        "        there_must_be_picture = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)  # I don't like it either\n",
        "        preprocessed_input = self.preprocess(text=text,images=there_must_be_picture, return_tensors=\"pt\")\n",
        "\n",
        "        # Encode text\n",
        "        outputs = self.model(**preprocessed_input)\n",
        "        return outputs.text_embeds"
      ],
      "metadata": {
        "id": "Emfd2cAXroja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BLIP2Retriever(Retriever):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.model, self.vis_processors, self.txt_processors = load_model_and_preprocess(name=\"blip2_feature_extractor\", model_type=\"pretrain_vitL\", is_eval=True, device=self.device)\n",
        "\n",
        "    def encode_images(self, images_paths, out_dir=None, batch_size=50):\n",
        "        batches  = [images_paths[i:i+batch_size] for i in range(0, len(images_paths), batch_size)]\n",
        "        self.image_IDs = []\n",
        "        self.image_encodings = None\n",
        "        for batch in batches:\n",
        "            images = []\n",
        "\n",
        "            # Preprocess Images\n",
        "            for image_name in batch:\n",
        "                images.append(Image.open(image_name).convert('RGB'))\n",
        "                self.image_IDs.append(image_name)\n",
        "\n",
        "            preprocessed_images = []\n",
        "            for image in images:\n",
        "                image_input = self.vis_processors[\"eval\"](image).unsqueeze(0).to(self.device)\n",
        "                preprocessed_images.append(image_input)\n",
        "            preprocessed_images = torch.cat(preprocessed_images)\n",
        "            del images\n",
        "\n",
        "            # Build sample\n",
        "            sample = {\"image\": preprocessed_images, \"text_input\": []}\n",
        "\n",
        "            # Encode Images\n",
        "            new_encodings = self.model.extract_features(sample, mode=\"image\").image_embeds[:,0,:]\n",
        "            if self.image_encodings is None:\n",
        "                self.image_encodings = new_encodings\n",
        "            else:\n",
        "                self.image_encodings = torch.cat((self.image_encodings, new_encodings), dim=0)\n",
        "\n",
        "            self.image_encodings = F.normalize(self.image_encodings, p=2, dim=-1)\n",
        "            # Save if out_dir specified\n",
        "            if out_dir is not None and self.image_encodings is not None:\n",
        "                torch.save(self.image_encodings, out_dir + '/BLIP2_image_encodings.pth')\n",
        "                with open(out_dir + '/BLIP2_images_IDs.pkl', 'wb') as f:\n",
        "                    pickle.dump(self.image_IDs, f)\n",
        "\n",
        "    def encode_text(self, text):\n",
        "        # Preprocess text\n",
        "        text = self.txt_processors[\"eval\"](text)\n",
        "\n",
        "        # Build sample\n",
        "        sample = {\"image\": torch.randn(1, 3, 50, 50), \"text_input\": [text]}\n",
        "\n",
        "        # Encode text\n",
        "        encoded_text = self.model.extract_features(sample, mode=\"text\").text_embeds[0,0,:]\n",
        "        encoded_text = torch.unsqueeze(encoded_text, dim=0)\n",
        "        return encoded_text\n"
      ],
      "metadata": {
        "id": "zSs6o5Hhrvqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files(directory):\n",
        "    return glob.glob(os.path.join(directory, \"*.jpeg\")) + glob.glob(os.path.join(directory, \"*.jpg\")) + glob.glob(os.path.join(directory, \"*.png\")) + glob.glob(os.path.join(directory, \"*.JPG\")) + glob.glob(os.path.join(directory, \"*.JPEG\"))\n",
        "\n",
        "def find_rank(list_of_tuples, ID):\n",
        "    for index, (cosine_similarity, Image_ID) in enumerate(list_of_tuples):\n",
        "        if Image_ID.endswith(ID):\n",
        "            return index\n",
        "    return -1  # Return -1 if ID not found in any Image_ID"
      ],
      "metadata": {
        "id": "vrHGJzChs_2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "!ls\n",
        "!cd drive\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w6nL4qpyPaQ",
        "outputId": "e069b8f4-cf07-4f6b-cd4a-80ea8a8a4e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "drive  sample_data\n",
            "cat: sample_data: Is a directory\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = BLIP2Retriever()\n",
        "images_dir = os.getcwd() + \"/drive/MyDrive/data\"\n",
        "save_dir = os.getcwd() + \"/drive/MyDrive/saves\"\n",
        "# r.encode_images(list_files(directory=images_dir),out_dir=save_dir)\n",
        "r.load_encoded_images(save_dir,model_name='BLIP2')\n",
        "\n",
        "\n",
        "ranks_cumulation = [0] * len(r.image_IDs)\n",
        "df = pd.read_csv(images_dir+'/labels.csv')\n",
        "for id,label in zip(df.ID,df.label):\n",
        "    ordered_images = r.compare_to_images(label)\n",
        "    rank = find_rank(ordered_images, id)\n",
        "    ranks_cumulation[rank:] = [x+1 for x in ranks_cumulation[rank:]]\n",
        "    # ranks_cumulation[rank] += 1\n",
        "\n",
        "plt.bar(range(len(ranks_cumulation[:50])),ranks_cumulation[:50])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "1puN_Ji-s1Dl",
        "outputId": "bcbfaeb6-4e71-4a58-b21c-97b4d3c3427c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}